
"""
LangGraph ì›Œí¬í”Œë¡œìš°
"""
from typing import List, Literal, AsyncGenerator
from typing_extensions import TypedDict

from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.tools import TavilySearchResults
from langgraph.graph import StateGraph, START, END
from pydantic import BaseModel, Field

from app.config import settings
from app.services.retriever import get_retriever_parallel
from app.services.generator import generate_answer, stream_generate_answer


# ============================================================
# State ì •ì˜
# ============================================================
class AgentState(TypedDict):
    query: str
    context: List[Document]
    answer: str
    is_web_search: bool


# ============================================================
# ë¬¸ì„œ ê´€ë ¨ì„± ì²´í¬ ìŠ¤í‚¤ë§ˆ
# ============================================================
class RelevanceScore(BaseModel):
    score: Literal[0, 1] = Field(
        description="0=ë¬¸ì„œë¡œ ë‹µë³€ ê°€ëŠ¥, 1=ë¬¸ì„œë¡œ ë‹µë³€ ë¶ˆê°€"
    )


# ============================================================
# ì „ì—­ ë³€ìˆ˜
# ============================================================
tavily_search_tool = None
relevance_chain = None
graph = None


# ============================================================
# ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™”
# ============================================================
def initialize_web_search():
    """ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global tavily_search_tool
    
    print("ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” ì¤‘...")
    
    tavily_search_tool = TavilySearchResults(
        max_results=settings.TAVILY_MAX_RESULTS,
        search_depth=settings.TAVILY_SEARCH_DEPTH,
        include_answer=True,
    )
    
    print("âœ… ì›¹ ê²€ìƒ‰ ë„êµ¬ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================
# ê´€ë ¨ì„± ì²´í¬ ì²´ì¸ ì´ˆê¸°í™”
# ============================================================
def initialize_relevance_chain():
    """ê´€ë ¨ì„± ì²´í¬ ì²´ì¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    global relevance_chain
    
    print("ê´€ë ¨ì„± ì²´í¬ ì²´ì¸ ì´ˆê¸°í™” ì¤‘...")
    
    from app.services.generator import llm
    
    if llm is None:
        raise RuntimeError("LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    
    relevance_system_prompt = """ë¬¸ì„œê°€ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ”ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‹µë³€ ê°€ëŠ¥(0): ë¬¸ì„œì— ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ êµ¬ì²´ì  ì •ë³´ê°€ ìˆìŒ
ë‹µë³€ ë¶ˆê°€(1): ë¬¸ì„œê°€ ì—†ê±°ë‚˜ ì§ˆë¬¸ê³¼ ë¬´ê´€í•˜ê±°ë‚˜ ì •ë³´ ë¶€ì¡±

ë¶ˆí™•ì‹¤í•˜ë©´ 0ì„ ë°˜í™˜í•˜ì„¸ìš”."""
    
    relevance_prompt = ChatPromptTemplate.from_messages([
        ('system', relevance_system_prompt),
        ('user', "ì§ˆë¬¸: {question}\n\në¬¸ì„œ:\n{documents}")
    ])
    
    structured_relevance_llm = llm.with_structured_output(RelevanceScore)
    relevance_chain = relevance_prompt | structured_relevance_llm
    
    print("âœ… ê´€ë ¨ì„± ì²´í¬ ì²´ì¸ ì´ˆê¸°í™” ì™„ë£Œ")


# ============================================================
# ë…¸ë“œ í•¨ìˆ˜ë“¤
# ============================================================
def retrieve_node(state: AgentState):
    """ë¬¸ì„œ ê²€ìƒ‰ ë…¸ë“œ"""
    print(f"\nğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘: {state['query']}")
    docs = get_retriever_parallel(state['query'])
    return {'context': docs, 'is_web_search': False}


def generate_node(state: AgentState):
    """ë‹µë³€ ìƒì„± ë…¸ë“œ"""
    context = state['context']
    is_web_search = state.get('is_web_search', False)
    
    print(f"\nâœï¸ ë‹µë³€ ìƒì„± ì¤‘ (ì›¹ê²€ìƒ‰: {is_web_search})")
    
    if not context:
        return {'answer': "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."}
    
    answer = generate_answer(state['query'], context, is_web_search)
    return {'answer': answer}


def web_search(state: AgentState) -> AgentState:
    """ì›¹ ê²€ìƒ‰ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    query = state['query']
    print(f"\nğŸŒ ì›¹ ê²€ìƒ‰ ì¤‘: {query}")
    results = tavily_search_tool.invoke(query)
    return {'context': results, 'is_web_search': True}


# ============================================================
# ì¡°ê±´ë¶€ ì—£ì§€ í•¨ìˆ˜
# ============================================================
def check_doc_relevance(state: AgentState) -> Literal['relevant', 'irrelevant']:
    """ë¬¸ì„œ ê´€ë ¨ì„±ì„ ì²´í¬í•©ë‹ˆë‹¤."""
    context = state['context']
    
    # 1. ë¬¸ì„œê°€ ì—†ìœ¼ë©´ irrelevant
    if not context:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ -> ì›¹ì„œì¹˜")
        return 'irrelevant'
    
    # 2. ë¬¸ì„œê°€ 2ê°œ ì´ìƒì´ë©´ relevant (ê°œì„ )
    if len(context) >= 2:
        print(f"âœ… ë¬¸ì„œ {len(context)}ê°œ ë°œê²¬ -> ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€")
        return 'relevant'
    
    # 3. ë¬¸ì„œê°€ 1ê°œì¼ ë•Œë§Œ LLMìœ¼ë¡œ ê´€ë ¨ì„± ì²´í¬
    try:
        response = relevance_chain.invoke({
            'question': state['query'], 
            'documents': context[:3]
        })
        
        result = 'relevant' if response.score == 0 else 'irrelevant'
        print(f"ğŸ“Š ê´€ë ¨ì„± ì ìˆ˜: {response.score} -> {result}")
        return result
        
    except Exception as e:
        # 4. ì˜ˆì™¸ ë°œìƒ ì‹œ ë¬¸ì„œê°€ ìˆìœ¼ë©´ relevantë¡œ ì²˜ë¦¬ (ê°œì„ )
        print(f"âš ï¸ ê´€ë ¨ì„± ì²´í¬ ì‹¤íŒ¨: {e} -> ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì‹œë„")
        return 'relevant'


# ============================================================
# ê·¸ë˜í”„ êµ¬ì¶•
# ============================================================
def build_graph():
    """LangGraph ì›Œí¬í”Œë¡œìš°ë¥¼ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    global graph
    
    print("LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì¤‘...")
    
    graph_builder = StateGraph(AgentState)
    
    graph_builder.add_node('retrieve_node', retrieve_node)
    graph_builder.add_node('generate_node', generate_node)
    graph_builder.add_node('web_search', web_search)
    
    graph_builder.add_edge(START, 'retrieve_node')
    graph_builder.add_conditional_edges(
        'retrieve_node',
        check_doc_relevance,
        {
            'irrelevant': 'web_search',
            'relevant': 'generate_node',
        }
    )
    graph_builder.add_edge('web_search', 'generate_node')
    graph_builder.add_edge('generate_node', END)
    
    graph = graph_builder.compile()
    
    print("âœ… LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì¶• ì™„ë£Œ")


# ============================================================
# ì‹¤í–‰ í•¨ìˆ˜
# ============================================================
def run_workflow(query: str) -> dict:
    """
    ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
    
    Returns:
        ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ê²°ê³¼ (answer, is_web_search í¬í•¨)
    """
    initial_state = {"query": query}
    result = graph.invoke(initial_state)
    
    return {
        'answer': result.get('answer', 'ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.'),
        'is_web_search': result.get('is_web_search', False)
    }


async def stream_workflow(query: str) -> AsyncGenerator[str, None]:
    """
    ì§ˆë¬¸ì„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•˜ì—¬ í† í° ë‹¨ìœ„ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
    
    Yields:
        ìƒì„±ë˜ëŠ” ë‹µë³€ í…ìŠ¤íŠ¸ ì¡°ê°
    """
    # 1. ë¬¸ì„œ ê²€ìƒ‰
    print(f"\nğŸ” ë¬¸ì„œ ê²€ìƒ‰ ì¤‘: {query}")
    docs = get_retriever_parallel(query)
    
    # 2. ë¬¸ì„œ ê´€ë ¨ì„± ì²´í¬
    is_web_search = False
    context = docs
    
    if not context:
        print("âš ï¸ ê²€ìƒ‰ëœ ë¬¸ì„œ ì—†ìŒ -> ì›¹ì„œì¹˜")
        is_web_search = True
        context = tavily_search_tool.invoke(query)
    elif len(context) >= 2:
        print(f"âœ… ë¬¸ì„œ {len(context)}ê°œ ë°œê²¬ -> ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€")
    else:
        # ë¬¸ì„œê°€ 1ê°œì¼ ë•Œë§Œ ê´€ë ¨ì„± ì²´í¬
        try:
            response = relevance_chain.invoke({
                'question': query, 
                'documents': context[:3]
            })
            
            if response.score == 1:
                print("ğŸ“Š ê´€ë ¨ì„± ë‚®ìŒ -> ì›¹ì„œì¹˜")
                is_web_search = True
                context = tavily_search_tool.invoke(query)
            else:
                print("ğŸ“Š ê´€ë ¨ì„± ì¶©ë¶„ -> ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€")
        except Exception as e:
            print(f"âš ï¸ ê´€ë ¨ì„± ì²´í¬ ì‹¤íŒ¨: {e} -> ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€ ì‹œë„")
    
    # 3. ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„±
    print(f"\nâœï¸ ë‹µë³€ ìƒì„± ì¤‘ (ì›¹ê²€ìƒ‰: {is_web_search})")
    
    if not context:
        yield "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        return
    
    async for chunk in stream_generate_answer(query, context, is_web_search):
        yield chunk


# ============================================================
# ì´ˆê¸°í™” í•¨ìˆ˜
# ============================================================
def initialize_workflow():
    """ì›Œí¬í”Œë¡œìš°ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    initialize_web_search()
    initialize_relevance_chain()
    
    # retriever_chain ì´ˆê¸°í™” (LLM ì˜ì¡´)
    from app.services.retriever import setup_retriever_chain
    setup_retriever_chain()
    
    build_graph()
    print("âœ… ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì™„ë£Œ\n")